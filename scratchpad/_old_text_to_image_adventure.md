# Text-to-Image Adventure Companion

A version of the AI gaming pal that augments Zork with inline scene illustrations generated by a text-to-image (T2I) model, presented in a Tkinter GUI alongside the transcript and narrator.

Top 3 objectives
- Build a robust text-to-image inference pipeline driven by the existing structured outputs (non-redundant, non-spoilery, in-world).
	- Prompt builder uses: game-intent, game-last-objects, game-last-changes, and game-room-path to form a compact, spoiler-safe scene prompt.
	- Determinism & style: seeded generation, negative prompts, and style presets (e.g., monochrome ink, retro CRT, line-art) for consistency across turns.
	- Safety & guardrails: filter disallowed content, avoid spoilers/invented objects, enforce “complementary not creative” guideline.

- Integrate a responsive Tkinter image viewport with caching and user controls.
	- Non-blocking generation (thread/async) so text UI remains smooth; visual loading state.
	- Controls: regenerate, style select, size presets, on/off toggle (hide image panel).
	- Caching & reuse: on-disk cache with seed+prompt hash; quick re-display on back/forward; file naming and sidecar metadata.

- Establish evaluation, logging, and batch generation to iterate quality.
	- Batch tool: iterate `res/test_run/player/*.jsonl` to pre-generate images per transaction/scene; write to `res/test_run/out/images/`.
	- Log metadata: prompt, negative prompt, seed, model, sampler, inference time; link back to the JSONL line and calling function.
	- Quality rubric: coherence with visible text, non-spoiler, non-redundant enhancement, style consistency, latency under target.

Meta objective
- Deliver an extensible visual companion that enriches (not replaces) the classic text-adventure experience with context-aware images generated from T2I models. The system turns the narrator’s structured understanding of the scene into consistent, spoiler-safe illustrations, decoupled from any single model backend (local or cloud), with reproducibility (seeds), caching, responsive UI, and auditable logs—so we can tune art style, performance, and safety while remaining faithful to Zork’s in-world tone.

Prompt template v1 (minimal)
- System/style preset: “monochrome ink illustration, retro text-adventure vibe, no UI chrome, no text overlays, avoid anachronisms.”
- Scene core (ordered, concise):
	- Location: last room name from recent output.
	- Objects: top 3 from game-last-objects (nouns only, no actions), omit invisible/secret items.
	- Change highlights: 1–2 from game-last-changes if they alter the scene physically.
	- Mood: one adjective from game-intent (e.g., dim, cramped, ominous).
- Negative prompt: spoilers, puzzle solutions, inventory not on-screen, faces close-up, photorealism, modern devices, text.
- Seed: per-turn deterministic seed derived from room name + turn index.

Caching & file structure
- `res/images/{player}/{turn:04d}_{room}_{seed}.png`
- `res/images/{player}/{turn:04d}_{room}_{seed}.json` (sidecar with prompt, neg, seed, model, latency)

UI integration principle
- GUI looks good but is purpose-built and minimal—we’re adding it only to display images.
- Provide a single optional image pane with spinner while generating.
- Controls: Regenerate, Style (dropdown), Size (S/M/L), Hide.
- Keyboard shortcuts: `i` toggle image, `r` regenerate.

Risks & mitigations
- Hallucination / spoilers: prompt builder only uses surfaced objects; strict negative prompts; small temperature/CFG.
- Latency: smaller models, lower resolution first, progressive refine on idle; cache hits preferred.
- Style drift: fixed presets + seeds; periodic evaluation against style checklist.

Leverage existing code & hooks
- Entry points: `zork_print`, `zork_input`, `collect_printed_messages`, `INTERACTIONS`, `stream_to_ui`.
- UI hooks to add in `zork_ui.py` (minimal): `show_image(image_or_path)`, `hide_image()`, `set_image_status(text)`.
- Orchestration in `zork_io.py`: trigger T2I only after `collect_printed_messages()` and before `read_prompt`, mirroring the narrator timing.
- Do not fork flows; reuse `MAX_LOG_LINES`, `config.json`, and `log/ai.jsonl` for auditing prompts and outputs.

Modules and non-destructive integration
- `t2i/image_service.py`: backend-agnostic interface (generate(prompt, seed, preset) -> PIL.Image | path) with adapters for local/cloud models.
- `t2i/prompt_builder.py`: spoiler-safe prompt construction from `response_schema.json` fields and recent `INTERACTIONS`.
- `t2i/cache.py`: deterministic paths, read/write image and sidecar metadata; seed/prompt hashing.
- `t2i/runner.py`: async orchestration; listens to hooks, calls `image_service`, stores results, emits events to UI.
- `image_companion_gui.py` (optional): minimal Tkinter window to display the latest scene image; no changes to console UI.
- `zork_t2i_integration.py`: thin glue that wires `zork_io` hooks to `t2i/runner` when enabled.
- Config keys: `image_helper_module_name`, `image_helper_module_path` (absolute!), `image_guidance_scale`, `image_inference_steps`, `image_log_file`, `image_use_fp16`, `image_show_progress`.
	- If `image_helper_module_path` is blank we auto-check `../Learning/text-to-image/{module}.py` relative to repo root.

Launch modes (non-destructive)
- Default: existing text UI only; no GUI, no image generation.
- Optional: enable via CLI flag or config (e.g., `--image-companion` or `"enable_image_companion": true`) to start `image_companion_gui` and T2I pipeline.
- Failure-tolerant: if T2I fails, the game and narrator continue unaffected.

Next steps
1) Pick initial backend (local SDXL/Flux or service) and wrap a thin `ImageService` interface.
2) Implement prompt builder tied to existing `response_schema.json` outputs (non-spoilery nouns only).
3) Add minimal image hooks in `zork_ui.py` and call them from `zork_io.zork_input` after narrator streaming (async, cached).

Gaps to close (with conclusions)
- Config/CLI switch
	- Add a config flag `enable_image_companion` and CLI `--image-companion` to toggle T2I.
	- Conclusion: Text UI remains default; GUI/T2I is opt-in and failure-tolerant.

- Minimal UI hooks
	- Implement `show_image(image_or_path)`, `hide_image()`, `set_image_status(text)` in `zork_ui.py` as optional/no-op when disabled.
	- Conclusion: Image display is isolated; no changes to core rendering flow.

- Integration glue
	- `zork_t2i_integration.py` listens after `collect_printed_messages()` and before `read_prompt`, mirroring narrator timing and respecting `MAX_LOG_LINES`.
	- Conclusion: Straightforward insertion point; zero impact on parsing/game loop.

- ImageService adapter + dependencies
	- Choose backend (local SDXL/Flux or cloud); define `ImageService.generate(prompt, seed, preset)` returning `PIL.Image | path`.
	- Provide absolute `image_helper_module_path` so no PYTHONPATH hacks.
	- Conclusion: Backend-agnostic, easy to swap/disable without touching UI or IO.

- Async and timeouts
	- Run T2I off the main thread/process; enforce timeouts; on error show status and continue.
	- Conclusion: Narrator and gameplay stay responsive regardless of image latency.

- Caching
	- Define cache dir (`res/images/{player}/`), deterministic filenames, and sidecar JSON metadata (prompt, seed, model, latency).
	- Conclusion: Fast re-display and reproducible generation; easy cleanup via .gitignore.

- Logging/eval
	- Log prompt/seed/model linkage to JSONL line and calling function; add a batch tool for `*.jsonl`.
	- Define a quality rubric (coherence, non-spoiler, style consistency, latency).
	- Conclusion: Auditable and tunable pipeline aligned with existing logging.

- README launch docs
	- Document flags, defaults, failure-tolerant behavior, and non-destructive principle.
	- Conclusion: Clear usage; developers/users can opt-in without surprises.

## Architectural Reality Check

- **Config/CLI plumbing is absent**: `config.json` is read ad hoc and `zork_config.py` only exposes a dict getter without loading values. Before any GUI/T2I flag exists, we must add a proper loader (e.g., argparse + config merge) so every subsystem can rely on a canonical `Settings` object.
- **Current UI is Rich-only**: `zork_ui.RichZorkUI` is tightly coupled to the console layout and `msvcrt.getwch` input loop. There is no abstraction layer to plug in a Tkinter pane, so we need to introduce an observer/event interface that can feed both the Rich view and the forthcoming GUI companion.
- **No T2I scaffolding yet**: none of the proposed `t2i/` modules, caches, or asset directories exist. The plan must treat these as net-new packages rather than assuming they are stubs.
- **Main-loop timing constraints**: `zork_io.zork_input` flushes printed text, streams narrator output, and blocks for input each turn. Image work must be triggered between `collect_printed_messages()` and `read_prompt()` and run asynchronously so the narrator never stalls.

## GUI Experience Emphasis

- **Dual-UI lifecycle**: document how the Rich console remains primary for text I/O while a Tkinter `image_companion_gui` window mirrors the latest scene thumbnail. The Tkinter process/thread should launch only when the feature flag is enabled and shut down cleanly when the game exits.
- **Focus and shortcuts**: define how keyboard input remains bound to the console (since `msvcrt` is blocking) while the GUI exposes mouse/keyboard shortcuts (`i` to toggle, `r` to regenerate) via Tkinter bindings, plus fallback buttons.
- **Event flow**: outline a queue or pub/sub channel where `zork_io` publishes scene contexts. The Tkinter pane subscribes to status updates (loading, cache hit, error) and image payloads without touching the main thread.
- **Success metrics tied to GUI**: add UX-focused KPIs—image pane responsiveness (target <150 ms to show cached images), regeneration latency feedback, graceful degradation when T2I fails—so evaluation stays aligned with “show the scene” as the primary objective.
 
## Current Build Status

- `--image-companion` CLI flag and matching `enable_image_companion` config property now gate the Tkinter companion window; console-only mode remains default.
- `ImageCompanionGUI` launches on demand and currently surfaces prompt text, negative prompt, and deterministic seed while we finalize the actual model adapter.
- `t2i.prompt_builder` consumes the narrator’s structured JSON (`game-room-path`, `game-last-objects`, `game-last-changes`, `game-intent`) to assemble spoiler-safe prompts plus seeds.
- `zork_t2i_integration` wires the main loop hooks: after narrator streaming and before awaiting player input, we queue the next scene prompt; the actual image generation call is stubbed pending backend selection.

## Phased Implementation Plan


### Phase 1 – Feature Flag, Config, and Core Hooks
1. [ ] Implement the `enable_image_companion` config flag (default `false`) plus matching CLI switch `--image-companion`, ensuring precedence is documented and validated early in `zork_config.py` so the rest of the stack can check a single canonical toggle.
2. [ ] Introduce no-op `show_image`, `hide_image`, and `set_image_status` functions in `zork_ui.py` with docstrings, and gate their Tkinter wiring behind the new toggle so that console-only runs incur zero overhead.
3. [ ] Update `zork_io.py` (or `zork_t2i_integration.py` if already separated) to read the toggle once per session, register lightweight callbacks around `collect_printed_messages()`, and emit trace logs when image generation requests are queued or skipped.

### Phase 2 – Prompt Builder and Deterministic Caching
1. [ ] Build `t2i/prompt_builder.py` with dataclasses that ingest `response_schema.json` fields (`game-intent`, `game-last-objects`, `game-last-changes`, `game-room-path`) and assemble the spoiler-safe prompt plus negative prompts exactly as documented above.
2. [ ] Implement deterministic seed derivation (`room name + turn index`) and expose it via the prompt builder so downstream services can stay stateless; include unit-level helpers for hashing and validation.
3. [ ] Create `t2i/cache.py` that hashes the prompt+seed, resolves paths like `res/images/{player}/{turn:04d}_{room}_{seed}.png`, and persists both PNG and JSON sidecars with prompt/seed/model metadata, handling cache hits before any T2I call.

### Phase 3 – Image Service Abstraction and Runner
1. [ ] Define `t2i/image_service.py` with an `ImageService` protocol or base class exposing `generate(prompt: str, seed: int, preset: str) -> ImageResult` along with adapters for the chosen backend (e.g., local SDXL via InvokeAI or a cloud API) and add `Pillow` to `requirements.txt`.
2. [ ] Implement `t2i/runner.py` that receives scene events, consults the cache, dispatches to `ImageService` on a worker thread or asyncio task, enforces timeouts, and translates success/failure into UI-friendly events (image ready, status text, errors logged).
3. [ ] Wire `zork_t2i_integration.py` (or equivalent glue) so that after `collect_printed_messages()` it packages context, invokes the runner asynchronously, and keeps the main narrator loop responsive even when generation is slow or fails.

### Phase 4 – GUI Pane and User Controls
1. [ ] Implement `image_companion_gui.py` (or extend `zork_ui.py`) with a Tkinter frame containing the image viewport, loading spinner/state label, and controls for regenerate, style dropdown, size presets, and hide/show toggle, all respecting the global flag.
2. [ ] Add keyboard shortcuts (`i` to toggle visibility, `r` to regenerate) plus accessibility affordances (focus order, fallback text) so keyboard-only users can manage the pane without interfering with the terminal flow.
3. [ ] Ensure the GUI listens to runner events (e.g., via thread-safe queue or `queue.Queue`) and reuses cached images instantly when navigating back/forward through turns, while disposing of stale images to avoid memory leaks.

### Phase 5 – Batch Generation, Logging, and Evaluation
1. [ ] Extend logging (`zork_logging.py` and `log/ai.jsonl`) to append prompt, negative prompt, seed, model, sampler, inference time, cache status, and linkage to the originating JSONL line for every generation attempt.
2. [ ] Build a batch CLI (e.g., `python zork_ai_eval.py --t2i-batch res/test_run/player/*.jsonl`) that iterates transcripts, generates/caches images into `res/test_run/out/images/`, and writes consolidated metadata for offline review.
3. [ ] Document a quality rubric and QA checklist (coherence, non-spoiler, latency targets, style consistency) in `README.md`, alongside troubleshooting steps and guidance for swapping backends, so iteration and evaluation remain repeatable.
